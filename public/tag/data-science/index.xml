<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Data Science | Stanley Z. Hua</title>
    <link>/tag/data-science/</link>
      <atom:link href="/tag/data-science/index.xml" rel="self" type="application/rss+xml" />
    <description>Data Science</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Sat, 10 Oct 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/img/me</url>
      <title>Data Science</title>
      <link>/tag/data-science/</link>
    </image>
    
    <item>
      <title>Principle Component Analysis for Dummies</title>
      <link>/post/pca/pca_for_dummies/</link>
      <pubDate>Sat, 10 Oct 2020 00:00:00 +0000</pubDate>
      <guid>/post/pca/pca_for_dummies/</guid>
      <description>&lt;style&gt;
body {background-color: #476e52 !important;
      font-size: 80%}
h3,h4{color: #e3e0bc !important;}
.note{font-size: 10pt;
      line-height: 20pt;
      padding-bottom: 10px}
p{text-indent: 2em;}

details{font-size: 10pt;}
summary{font-size: 100% !important;}
&lt;/style&gt;
&lt;div class=&#34;note&#34;&gt;
&lt;b&gt;DISCLAIMER&lt;/b&gt;: This article is an introduction into PCA and does not go into depth into the statistics and code. Tread carefully! 
&lt;/div&gt;
&lt;hr&gt;
&lt;h3&gt;OVERVIEW&lt;/h3&gt;
&lt;p&gt;PCA or Principle Component Analysis is known for two things: 1) Dimensionality Reduction, and 2) Structure Analysis. However more generally, it is used for &lt;strong&gt;factor extraction&lt;/strong&gt;, which is needed in Factor Analysis.&lt;/p&gt;
&lt;div class=&#34;note&#34;&gt;
&lt;b&gt;NOTE&lt;/b&gt;: There are plenty of resources online if you wish to learn more about Factor Analysis, but they will not be covered here!
&lt;/div&gt;
&lt;br&gt;
&lt;h4&gt;TERMINOLOGY&lt;/h4&gt;
&lt;p&gt;So as not to lose anyone, let&amp;rsquo;s define some useful terminology! 
&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Features&lt;/strong&gt; are what we are measuring.&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;Example&lt;/summary&gt;
&lt;p&gt;Say we have a table with &lt;i&gt;y&lt;/i&gt; rows and &lt;i&gt;x&lt;/i&gt; columns, then we have &lt;i&gt;x&lt;/i&gt; features. Imagine each row being the names of someone you know, and each column measure something distinct about all persons listed (e.g. their height, weight, deepest darkest secrets, etc.). These things we&#39;re measuring are the features.&lt;/p&gt;
&lt;/details&gt;
&lt;br&gt;
&lt;p&gt;&lt;strong&gt;Dimensionality Reduction&lt;/strong&gt; is exactly what it sounds like. The goal is to reduce the number of dimensions (i.e. number of features), while retaining &lt;em&gt;most&lt;/em&gt; information from the original data. Using this to understand underlying structures in the data is the idea behind &lt;strong&gt;Structure Analysis&lt;/strong&gt;.&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;Example and Enrichment&lt;/summary&gt;
&lt;p&gt;Say you wish to visualize your data, in order to get an understanding of the relationships between each feature. However, you have too many features. It becomes impossible to plot them on an x-y graph. How do you visualize this without destroying your computer let alone the laws of physics? Simple, you reduce the number of dimensions to 2. Now, you can plot it on a coordinate plane! &lt;/p&gt;
&lt;div class=&#34;note&#34;&gt;
&lt;b&gt;NOTE&lt;/b&gt;: Dimensionality Reduction is different from &lt;i&gt;Feature Selection&lt;/i&gt; (e.g. L1 Regularization, L2 Regularization, etc.). The goal of feature selection is to select the most important features. Meanwhile, dimensionality reduction is used to lessen the number of dimensions while capturing the variation in the original data as much as possible.
&lt;/div&gt;
&lt;/details&gt;
&lt;br&gt;
&lt;p&gt;&lt;strong&gt;Principal Components&lt;/strong&gt; are the [1, n] factors extracted from n features using Principal Component Analysis. You can think of them as new axes to view the original data. &lt;em&gt;However, you can no longer interpret the principal components the same way you did with previous features&lt;/em&gt;.&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;Enrichment&lt;/summary&gt;
If you have 10 features and you do PCA to get 10 factors, they are not the same. If you previously had age, weight, etc., now you simply have axes for data points with no semantic meaning.
&lt;/details&gt;
&lt;br&gt;&lt;br&gt;
&lt;h3&gt;GENERAL IDEA&lt;/h3&gt;
&lt;p&gt;To build intuition about what happens in Principal Component Analysis, let&amp;rsquo;s use the graph below as an example.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/PCA/2020-10-10-pca-for-dummies.en_files/figure-html/plot-1.png&#34; width=&#34;336&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Here, we have 2 features. You can imagine each point representing an observation (e.g. one of your participants).&lt;/p&gt;
&lt;p&gt;When you implement PCA, imagine a line placed at the center (of all the data points). Imagine rotating it, and every time that you rotate it, it changes how far each observation (i.e. its projection onto the line) point is from the center of the line. This is also called the &lt;strong&gt;squared distances&lt;/strong&gt;. And this is what we&amp;rsquo;re trying to &lt;em&gt;maximize&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;By doing so, it is as if we are trying to find a line where the observations are farthest from center. Thus, we capture the most amount of variation along this line, and this line is known as a principal component!&lt;/p&gt;
&lt;p&gt;Once again, notice how the features no longer have anything to do with the new axes (principal component). Also notice that if you had 3 or more features, graphing it like we did isn&amp;rsquo;t possible.&lt;/p&gt;
&lt;h3&gt;Additional Resources&lt;/h3&gt;
&lt;p&gt;StatQuest has great videos explaining what I mentioned with the graph with more detail!&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
